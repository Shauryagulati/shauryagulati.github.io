



## *Evaluating Prompting Techniques Across Large Language Models*

### • GPT-5, Gemini Pro, Gemma3:4b  
### • Prompt Engineering, LLM Evaluation, Comparative Analysis  

Conducted a structured evaluation of three Large Language Models (GPT-5, Gemini Pro, and Gemma3:4b) across three developer-focused tasks — Code Summarization, API Question Answering, and API Documentation Generation.  
Each task was tested using **CLEAR**, **Few-Shot**, and **Chain-of-Thought (CoT)** prompting strategies, producing 27 total outputs for comparison.  

Outputs were blindly scored on Accuracy, Completeness, Coherence, and Domain Appropriateness using a 4-point rubric.  
**Findings:** GPT-5 and Gemini Pro tied with an average score of **3.95**, outperforming Gemma3:4b (3.64). CoT prompting delivered the best reasoning depth, while CLEAR prompts excelled at concise, reliable outputs.  

**Key Insights:**
- GPT-5 and Gemini Pro are ideal for production-grade reasoning workflows.  
- Gemma3:4b offers impressive speed and cost-efficiency for prototyping.  
- CLEAR + CoT combination yields optimal balance between reasoning and precision.  

This assignment deepened my understanding of **how prompt design impacts model performance**, **bias control**, and **LLM reasoning trade-offs**.  
